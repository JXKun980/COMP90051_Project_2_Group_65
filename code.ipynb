{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0b8024ca-de50-48a7-83ba-0df4d19842e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336bfd7c-b9bb-4c83-a26b-393f8f183ae6",
   "metadata": {},
   "source": [
    "# 1. Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ce63ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyexpat.errors import XML_ERROR_FEATURE_REQUIRES_XML_DTD\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "NUM_PROLIFIC_AUTHORS = 100\n",
    "NUM_WORDS = 5000\n",
    "NUM_VENUES = 466 # 465 valid venues + 1 reserved for null venues\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_name, data_dir, transform=None, target_transform=None):\n",
    "        # Get data\n",
    "        self.data_dir = data_dir\n",
    "        self.data = pd.read_json(os.path.join(data_dir, file_name))\n",
    "\n",
    "        # Clean data\n",
    "        self.data.venue = self.data.venue.map(lambda x: 465 if x == '' else x) # Change \"\" data to 465, a new cateogry\n",
    "\n",
    "        # # Convert venue to one-hot as it is not ordinal data, and convert to numpy array\n",
    "        # venue = np.zeros((len(self), NUM_VENUES))\n",
    "        # venue[np.arange(len(self)), self.data.venue] = 1\n",
    "\n",
    "        # Convert year and venue to numpy array\n",
    "        self.x_year = self.data.year.to_numpy()[:,np.newaxis]\n",
    "        self.x_venue = self.data.venue.to_numpy()[:,np.newaxis]\n",
    "\n",
    "        # Convert abstracts and titles to lists of arrays of words (in numbers)\n",
    "        abstracts_list = self.data.abstract.to_list()\n",
    "        self.x_abstract = [np.array(x)[:,np.newaxis] for x in abstracts_list]\n",
    "        titles_list = self.data.title.to_list()\n",
    "        self.x_title = [np.array(x)[:,np.newaxis] for x in titles_list]\n",
    "\n",
    "        # Convert y to numpy array\n",
    "        self.y = self.data.authors.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_year = self.x_year[idx]\n",
    "        x_venue = self.x_venue[idx]\n",
    "        x_abstract = self.x_abstract[idx]\n",
    "        x_title = self.x_title[idx]\n",
    "        y = self.authors_to_one_hot(self.y[idx])\n",
    "        return x_year, x_venue, x_abstract, x_title, y\n",
    "\n",
    "    def authors_to_one_hot(self, authors):\n",
    "        one_hot = np.zeros(NUM_PROLIFIC_AUTHORS).astype(int)\n",
    "        a_array = np.array(authors)\n",
    "        prolific_a_array = a_array[a_array < 100]\n",
    "        one_hot[prolific_a_array] = 1\n",
    "        return one_hot\n",
    "\n",
    "# def text_to_one_hot(text):\n",
    "#     '''\n",
    "#     Take a piece of text (represented as list of words represented in numbers) and output \n",
    "#     a numpy matrix representing the texts, where each row represent a word in one-hot encoded format\n",
    "#     '''\n",
    "#     t_array = np.zeros((len(text), NUM_WORDS))\n",
    "#     t_array[np.arange(len(text)), text] = 1\n",
    "#     return t_array\n",
    "\n",
    "\n",
    "dataset = CustomDataset('train.json', './data/')\n",
    "train_set, val_set = random_split(dataset, [round(len(dataset)*0.7), round(len(dataset)*0.3)])\n",
    "\n",
    "def pad_collate(batch):\n",
    "    (x_year, x_venue, x_abstract, x_title, y) = zip(*batch)\n",
    "    # print(torch.tensor(x_fixed))\n",
    "    \n",
    "    # Convert x (year and venue) and y to tensors and pack them\n",
    "    x_year_tensor = torch.tensor(x_year)\n",
    "    x_venue_tensor = torch.tensor(x_venue)\n",
    "    y_tensor = torch.tensor(y)\n",
    "\n",
    "    # Convert x_title and x_abstract to tensors and pack them\n",
    "\n",
    "    # Get length to later recover orignal from padded tensors\n",
    "    x_abstract_lens = [len(x) for x in x_abstract]\n",
    "    x_title_lens = [len(x) for x in x_title]\n",
    "\n",
    "    # Convert words to tensors\n",
    "    x_abstract_tensor = [torch.tensor(x) for x in x_abstract]\n",
    "    x_title_tensor = [torch.tensor(x) for x in x_title]\n",
    "\n",
    "    # Pad variable length tensors\n",
    "    x_abstract_pad = pad_sequence(x_abstract_tensor, padding_value=0)\n",
    "    x_title_pad = pad_sequence(x_title_tensor, padding_value=0)\n",
    "\n",
    "    # Pack padded tensors along with lengths for recovery\n",
    "    x_abstract_packed = pack_padded_sequence(x_abstract_pad, x_abstract_lens, enforce_sorted=False)\n",
    "    x_title_packed = pack_padded_sequence(x_title_pad, x_title_lens, enforce_sorted=False)\n",
    "\n",
    "    return x_year_tensor, x_venue_tensor, x_abstract_packed, x_title_packed, y_tensor\n",
    "    \n",
    "train_dataloader = DataLoader(train_set, batch_size=128, shuffle=True, collate_fn=pad_collate)\n",
    "val_dataloader = DataLoader(val_set, batch_size=128, shuffle=True, collate_fn=pad_collate)\n",
    "\n",
    "# for _ in range(5):\n",
    "#     _, _, _, y = next(iter(train_dataloader))\n",
    "#     print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16bcd0b-84f1-4844-90f6-bbe517305f23",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a8483",
   "metadata": {},
   "source": [
    "## 2.1 NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f683457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "'''\n",
    "Define the NN model tailored for the task\n",
    "'''\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, embed_input_size, other_input_size, embed_vocab, embed_size, \n",
    "        hidden_size1, hidden_size2, out_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_layer = nn.Embedding(embed_vocab, embed_size)\n",
    "        self.input_layer = nn.Linear(embed_size*embed_input_size + other_input_size, hidden_size1)\n",
    "        self.hidden_layer = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.output_layer = nn.Linear(hidden_size2, out_size)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x_embed, x):\n",
    "        batch_size = x_embed.shape[0]\n",
    "        embed_out = self.embedding_layer(x_embed)\n",
    "        embed_out = torch.flatten(embed_out, start_dim=1) # Flatten the new dimension added by embedding\n",
    "        # print(embed_out.shape)\n",
    "\n",
    "        x = torch.cat((embed_out, x), 1)\n",
    "        \n",
    "        x = F.relu(self.input_layer(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.hidden_layer(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        out = F.sigmoid(self.output_layer(x))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb92d44",
   "metadata": {},
   "source": [
    "## 2.2 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "23f8a64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "908463ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_MLP(nn.Module):\n",
    "    def __init__(self, rnn_embed_vocab, rnn_embed_size, rnn_hidden_size,\n",
    "        mlp_embed_input_size, mlp_other_input_size, mlp_embed_vocab, \n",
    "        mlp_embed_size, mlp_hidden_dim1, mlp_hidden_dim2, mlp_out_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.embedding_layer = nn.Embedding(rnn_embed_vocab, rnn_embed_size)\n",
    "        self.rnn1 = nn.RNN(\n",
    "            input_size=1, \n",
    "            hidden_size=rnn_hidden_size,\n",
    "            nonlinearity='tanh',\n",
    "            )\n",
    "        self.rnn2 = nn.RNN(\n",
    "            input_size=1, \n",
    "            hidden_size=rnn_hidden_size,\n",
    "            nonlinearity='tanh',\n",
    "            )\n",
    "        self.mlp = MLP(\n",
    "            mlp_embed_input_size, \n",
    "            mlp_other_input_size + rnn_hidden_size*2, \n",
    "            mlp_embed_vocab, \n",
    "            mlp_embed_size, \n",
    "            mlp_hidden_dim1, \n",
    "            mlp_hidden_dim2, \n",
    "            mlp_out_size)\n",
    "\n",
    "    def forward(self, x_rnn1, x_rnn2, x_mlp_embed, x_mlp):\n",
    "        # print(x_rnn1.shape)\n",
    "        # emed_out1 = self.embedding_layer(x_rnn1)\n",
    "        rnn_out1 = self.rnn1(x_rnn1)\n",
    "\n",
    "        # emed_out2 = self.embedding_layer(x_rnn2)\n",
    "        rnn_out2 = self.rnn2(x_rnn2)\n",
    "\n",
    "        mlp_out = self.mlp(x_mlp_embed, torch.cat((rnn_out1, rnn_out2, x_mlp), 1))\n",
    "        \n",
    "        return mlp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1405f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size) \n",
    "        self.activation = nn.Tanh() \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.activation(self.i2h(combined)) \n",
    "        output = self.h2o(hidden) \n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "42abed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionalGRUClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(AttentionalGRUClassifier, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size) \n",
    "        self.att = nn.Linear(hidden_size, 1) \n",
    "        \n",
    "    def forward(self, input_sequence):\n",
    "        # process the input sequence into a sequence of RNN hidden states\n",
    "        states, _ = self.gru(input_sequence)\n",
    "        # compute attention scores to each RNN hidden state (we use a linear function)\n",
    "        att_scores = self.att(states)\n",
    "        # rescale the attention scores using a softmax, so they sum to one\n",
    "        alpha = F.softmax(att_scores, dim=0)\n",
    "        # compute the \"c\" vector as a weighted combination of the RNN hidden states\n",
    "        c = torch.sum(torch.mul(states, alpha), dim=0)\n",
    "        # now couple up the c state to the output, and compute log-softmax\n",
    "        output = self.h2o(c.view(1, -1)) \n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41c4eb3",
   "metadata": {},
   "source": [
    "RNN Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "08285c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def categoryFromOutput(output):\n",
    "#     top_n, top_i = output.topk(1)\n",
    "#     category_i = top_i[0].item()\n",
    "#     return all_categories[category_i], category_i\n",
    "\n",
    "# def randomChoice(l):\n",
    "#     return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "# def randomTrainingExample(noise=0, noise_chars=\".,;'\"):\n",
    "#     # noise: integer denoting the maximum number of distractor characters to add\n",
    "#     # noise_chars: inventory of distractor characters\n",
    "#     category = randomChoice(all_categories)\n",
    "#     line = randomChoice(category_lines[category])\n",
    "#     # added code to insert distracting nonsense into the string\n",
    "#     if noise > 0:\n",
    "#         line_prime = line\n",
    "#         for i in range(random.randint(0, noise+1)):\n",
    "#             line_prime += random.choice(noise_chars)\n",
    "#         line = line_prime\n",
    "#     # end change\n",
    "#     category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "#     line_tensor = lineToTensor(line)\n",
    "#     return category, line, category_tensor, line_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6758a0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_iters = 80000\n",
    "# print_every = 5000\n",
    "# plot_every = 1000\n",
    "# noise_level = 0 # change this line (as discussed later)\n",
    "# n_hidden = 32\n",
    "# learning_rate = 0.005\n",
    "\n",
    "# current_loss = 0\n",
    "# all_losses = []\n",
    "\n",
    "# rnn = RNNClassifier(n_letters, n_hidden, n_categories)\n",
    "# criterion = nn.NLLLoss()\n",
    "\n",
    "# def timeSince(since):\n",
    "#     now = time.time()\n",
    "#     s = now - since\n",
    "#     m = math.floor(s / 60)\n",
    "#     s -= m * 60\n",
    "#     return '%dm %ds' % (m, s)\n",
    "# start = time.time()\n",
    "\n",
    "# # training algorithm, which takes one instance and performs single SGD update\n",
    "# def train(category_tensor, line_tensor):\n",
    "#     hidden = rnn.initHidden()\n",
    "#     rnn.zero_grad()\n",
    "#     # key step: unroll the RNN over each symbol in the input sequence\n",
    "#     for i in range(line_tensor.size()[0]):\n",
    "#         output, hidden = rnn(line_tensor[i], hidden)\n",
    "#     # treat the last output as the prediction of the category label\n",
    "#     loss = criterion(output, category_tensor)\n",
    "#     loss.backward()\n",
    "\n",
    "#     # Add parameters' gradients to their values, multiplied by learning rate\n",
    "#     for p in rnn.parameters():\n",
    "#         p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "#     return output, loss.item()\n",
    "\n",
    "# for iter in range(1, n_iters + 1):\n",
    "#     category, line, category_tensor, line_tensor = randomTrainingExample(noise=noise_level)\n",
    "#     output, loss = train(category_tensor, line_tensor)\n",
    "#     current_loss += loss\n",
    "\n",
    "#     # Print iter number, loss, name and guess\n",
    "#     if iter % print_every == 0:\n",
    "#         guess, guess_i = categoryFromOutput(output)\n",
    "#         correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "#         print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "#     # Add current loss avg to list of losses\n",
    "#     if iter % plot_every == 0:\n",
    "#         all_losses.append(current_loss / plot_every)\n",
    "#         current_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4269712c",
   "metadata": {},
   "source": [
    "GRU Traning code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9c2d9441",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# model = AttentionalGRUClassifier(n_letters, n_hidden, n_categories)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# criterion = nn.NLLLoss()\n",
    "\n",
    "# start = time.time()\n",
    "# all_losses_att = []\n",
    "# current_loss = 0\n",
    "\n",
    "# for iter in range(1, n_iters + 1):\n",
    "#     category, line, category_tensor, line_tensor = randomTrainingExample(noise=noise_level)\n",
    "\n",
    "#     model.zero_grad()\n",
    "#     output, _ = model.forward(line_tensor)\n",
    "#     output = torch.squeeze(output, 1) # remove redundant dimension\n",
    "#     loss = criterion(output, category_tensor)\n",
    "#     current_loss += loss.item()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # Print iter number, loss, name and guess\n",
    "#     if iter % print_every == 0:\n",
    "#         guess, guess_i = categoryFromOutput(output)\n",
    "#         correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "#         print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "#     # Add current loss avg to list of losses\n",
    "#     if iter % plot_every == 0:\n",
    "#         all_losses_att.append(current_loss / plot_every)\n",
    "#         current_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d722179",
   "metadata": {},
   "source": [
    "# 3. Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7703ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tkinter import Y\n",
    "\n",
    "def test(model, criterion, test_loader):\n",
    "    test_loss = 0.\n",
    "    test_preds, test_labels = list(), list()\n",
    "    for i, data in enumerate(test_loader):\n",
    "        x_year, x_venue, x_abstract, x_title, y = data\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(x_abstract, x_title, x_venue, x_year)\n",
    "            predictions = torch.round(logits)\n",
    "            test_loss += criterion(input=logits, target=y.float()).item()\n",
    "            test_preds.append(predictions)\n",
    "            test_labels.append(y)\n",
    "\n",
    "    test_preds = torch.cat(test_preds)\n",
    "    test_labels = torch.cat(test_labels)\n",
    "\n",
    "    test_accuracy = torch.mean((torch.sum(torch.eq(predictions, y).float(), 1) == 100).float()).item()\n",
    "\n",
    "    print('[TEST] Mean loss {:.4f} | Accuracy {:.4f}'.format(test_loss/len(test_loader), test_accuracy))\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, n_epochs=10):\n",
    "    \"\"\"\n",
    "    Generic training loop for supervised multiclass learning\n",
    "    \"\"\"\n",
    "    LOG_INTERVAL = 250\n",
    "    running_loss, running_accuracy = list(), list()\n",
    "    start_time = time.time()\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    for epoch in range(n_epochs):  # Loop over training dataset `n_epochs` times\n",
    "\n",
    "        epoch_loss = 0.\n",
    "\n",
    "        for i, data in enumerate(train_loader):  # Loop over elements in training set\n",
    "\n",
    "            x_year, x_venue, x_abstract, x_title, y = data\n",
    "\n",
    "            logits = model(x_abstract, x_title, x_venue, x_year)\n",
    "\n",
    "            predictions = torch.round(logits)\n",
    "            # print(f'{torch.mean((torch.sum(torch.eq(predictions, y).float(), 1) == 100).float())}\\n')\n",
    "            train_acc = torch.mean((torch.sum(torch.eq(predictions, y).float(), 1) == 100).float()).item()\n",
    "\n",
    "            # print(logits)\n",
    "            loss = criterion(input=logits, target=y.float())\n",
    "\n",
    "            loss.backward()               # Backward pass (compute parameter gradients)\n",
    "            optimizer.step()              # Update weight parameter using SGD\n",
    "            optimizer.zero_grad()         # Reset gradients to zero for next iteration\n",
    "\n",
    "\n",
    "            # ============================================================================\n",
    "            # You can safely ignore the boilerplate code below - just reports metrics over\n",
    "            # training and test sets\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "            running_accuracy.append(train_acc)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if i % LOG_INTERVAL == 0:  # Log training stats\n",
    "                deltaT = time.time() - start_time\n",
    "                mean_loss = epoch_loss / (i+1)\n",
    "                print('[TRAIN] Epoch {} [{}/{}]| Mean loss {:.4f} | Current Mean train accuracy {:.5f} | Current data accuracy {:.5f} | Time {:.2f} s'.format(epoch, \n",
    "                    i, len(train_loader), mean_loss, sum(running_accuracy)/len(running_accuracy), train_acc, deltaT))\n",
    "\n",
    "        print('Epoch complete! Mean loss: {:.4f}'.format(epoch_loss/len(train_loader)))\n",
    "\n",
    "        test(model, criterion, val_loader)\n",
    "        \n",
    "    return running_loss, running_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b3f3f637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Epoch 0 [0/142]| Mean loss 0.6996 | Current Mean train accuracy 0.00000 | Current data accuracy 0.00000 | Time 0.01 s\n",
      "Epoch complete! Mean loss: 0.2637\n",
      "[TEST] Mean loss 0.0391 | Accuracy 0.7241\n",
      "[TRAIN] Epoch 1 [0/142]| Mean loss 0.0448 | Current Mean train accuracy 0.45681 | Current data accuracy 0.61719 | Time 1.98 s\n",
      "Epoch complete! Mean loss: 0.0378\n",
      "[TEST] Mean loss 0.0354 | Accuracy 0.7586\n",
      "[TRAIN] Epoch 2 [0/142]| Mean loss 0.0418 | Current Mean train accuracy 0.58367 | Current data accuracy 0.64844 | Time 3.91 s\n",
      "Epoch complete! Mean loss: 0.0355\n",
      "[TEST] Mean loss 0.0340 | Accuracy 0.7069\n",
      "[TRAIN] Epoch 3 [0/142]| Mean loss 0.0342 | Current Mean train accuracy 0.62626 | Current data accuracy 0.72656 | Time 5.79 s\n",
      "Epoch complete! Mean loss: 0.0346\n",
      "[TEST] Mean loss 0.0329 | Accuracy 0.6552\n",
      "[TRAIN] Epoch 4 [0/142]| Mean loss 0.0332 | Current Mean train accuracy 0.64623 | Current data accuracy 0.70312 | Time 7.73 s\n",
      "Epoch complete! Mean loss: 0.0331\n",
      "[TEST] Mean loss 0.0322 | Accuracy 0.7241\n",
      "[TRAIN] Epoch 5 [0/142]| Mean loss 0.0324 | Current Mean train accuracy 0.65941 | Current data accuracy 0.71094 | Time 9.68 s\n",
      "Epoch complete! Mean loss: 0.0322\n",
      "[TEST] Mean loss 0.0314 | Accuracy 0.6897\n",
      "[TRAIN] Epoch 6 [0/142]| Mean loss 0.0340 | Current Mean train accuracy 0.66800 | Current data accuracy 0.68750 | Time 11.61 s\n",
      "Epoch complete! Mean loss: 0.0315\n",
      "[TEST] Mean loss 0.0305 | Accuracy 0.7241\n",
      "[TRAIN] Epoch 7 [0/142]| Mean loss 0.0298 | Current Mean train accuracy 0.67416 | Current data accuracy 0.67969 | Time 13.60 s\n",
      "Epoch complete! Mean loss: 0.0309\n",
      "[TEST] Mean loss 0.0300 | Accuracy 0.7931\n",
      "[TRAIN] Epoch 8 [0/142]| Mean loss 0.0344 | Current Mean train accuracy 0.67853 | Current data accuracy 0.66406 | Time 15.51 s\n",
      "Epoch complete! Mean loss: 0.0302\n",
      "[TEST] Mean loss 0.0291 | Accuracy 0.7931\n",
      "[TRAIN] Epoch 9 [0/142]| Mean loss 0.0281 | Current Mean train accuracy 0.68211 | Current data accuracy 0.74219 | Time 17.54 s\n",
      "Epoch complete! Mean loss: 0.0297\n",
      "[TEST] Mean loss 0.0287 | Accuracy 0.6897\n"
     ]
    }
   ],
   "source": [
    "mlp_model = MLP(\n",
    "    embed_input_size=1, \n",
    "    other_input_size=1, \n",
    "    embed_vocab=NUM_VENUES, \n",
    "    embed_size=NUM_VENUES//2, \n",
    "    hidden_size1=512, \n",
    "    hidden_size2=256, \n",
    "    out_size=100)\n",
    "    \n",
    "optimizer = torch.optim.SGD(mlp_model.parameters(), lr=1e-2, momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(mlp_model.parameters(), lr=1e-2)\n",
    "mlp_loss, mlp_acc = train(mlp_model, train_dataloader, val_dataloader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffcb323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter  # Smooth spiky curves\n",
    "import matplotlib.pyplot as plt\n",
    "running_loss_smoothed = savgol_filter(mlp_loss, 21, 3)\n",
    "running_acc_smoothed = savgol_filter(mlp_acc, 21, 3)\n",
    "\n",
    "plt.plot(running_loss_smoothed)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cross-entropy Loss (Train)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72119af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(running_acc_smoothed)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy (Train)')\n",
    "plt.ylim(0.2,1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "800df78d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input must have 2 dimensions, got 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\1-Google Drive\\1-Work\\Projects\\COMP90051\\Project2\\code.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/1-Google%20Drive/1-Work/Projects/COMP90051/Project2/code.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(mlp_model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-2\u001b[39m, momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/1-Google%20Drive/1-Work/Projects/COMP90051/Project2/code.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# optimizer = torch.optim.Adam(mlp_model.parameters(), lr=1e-2)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/1-Google%20Drive/1-Work/Projects/COMP90051/Project2/code.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m mlp_loss, mlp_acc \u001b[39m=\u001b[39m train(mlp_model, train_dataloader, val_dataloader, optimizer)\n",
      "\u001b[1;32mc:\\1-Google Drive\\1-Work\\Projects\\COMP90051\\Project2\\code.ipynb Cell 22\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, optimizer, n_epochs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/1-Google%20Drive/1-Work/Projects/COMP90051/Project2/code.ipynb#X30sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):  \u001b[39m# Loop over elements in training set\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/1-Google%20Drive/1-Work/Projects/COMP90051/Project2/code.ipynb#X30sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     x_year, x_venue, x_abstract, x_title, y \u001b[39m=\u001b[39m data\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/1-Google%20Drive/1-Work/Projects/COMP90051/Project2/code.ipynb#X30sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     logits \u001b[39m=\u001b[39m model(x_abstract, x_title, x_venue, x_year)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/1-Google%20Drive/1-Work/Projects/COMP90051/Project2/code.ipynb#X30sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     predictions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mround(logits)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/1-Google%20Drive/1-Work/Projects/COMP90051/Project2/code.ipynb#X30sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39m# print(f'{torch.mean((torch.sum(torch.eq(predictions, y).float(), 1) == 100).float())}\\n')\u001b[39;00m\n",
      "File \u001b[1;32mc:\\1-Environment\\miniconda\\envs\\COMP90051\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\1-Google Drive\\1-Work\\Projects\\COMP90051\\Project2\\code.ipynb Cell 22\u001b[0m in \u001b[0;36mRNN_MLP.forward\u001b[1;34m(self, x_rnn1, x_rnn2, x_mlp_embed, x_mlp)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/1-Google%20Drive/1-Work/Projects/COMP90051/Project2/code.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x_rnn1, x_rnn2, x_mlp_embed, x_mlp):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/1-Google%20Drive/1-Work/Projects/COMP90051/Project2/code.ipynb#X30sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m# print(x_rnn1.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/1-Google%20Drive/1-Work/Projects/COMP90051/Project2/code.ipynb#X30sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39m# emed_out1 = self.embedding_layer(x_rnn1)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/1-Google%20Drive/1-Work/Projects/COMP90051/Project2/code.ipynb#X30sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     rnn_out1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn1(x_rnn1)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/1-Google%20Drive/1-Work/Projects/COMP90051/Project2/code.ipynb#X30sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39m# emed_out2 = self.embedding_layer(x_rnn2)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/1-Google%20Drive/1-Work/Projects/COMP90051/Project2/code.ipynb#X30sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     rnn_out2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn2(x_rnn2)\n",
      "File \u001b[1;32mc:\\1-Environment\\miniconda\\envs\\COMP90051\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\1-Environment\\miniconda\\envs\\COMP90051\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:265\u001b[0m, in \u001b[0;36mRNNBase.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39massert\u001b[39;00m hx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m cast(Tensor, \u001b[39minput\u001b[39m)\n\u001b[1;32m--> 265\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[0;32m    266\u001b[0m _impl \u001b[39m=\u001b[39m _rnn_impls[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode]\n\u001b[0;32m    267\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\1-Environment\\miniconda\\envs\\COMP90051\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:229\u001b[0m, in \u001b[0;36mRNNBase.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, hidden: Tensor, batch_sizes: Optional[Tensor]):\n\u001b[1;32m--> 229\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_input(\u001b[39minput\u001b[39;49m, batch_sizes)\n\u001b[0;32m    230\u001b[0m     expected_hidden_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_hidden_size(\u001b[39minput\u001b[39m, batch_sizes)\n\u001b[0;32m    232\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden, expected_hidden_size)\n",
      "File \u001b[1;32mc:\\1-Environment\\miniconda\\envs\\COMP90051\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:201\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    199\u001b[0m expected_input_dim \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m3\u001b[39m\n\u001b[0;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m expected_input_dim:\n\u001b[1;32m--> 201\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    202\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput must have \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m dimensions, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    203\u001b[0m             expected_input_dim, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n\u001b[0;32m    204\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m    205\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    206\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    207\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input must have 2 dimensions, got 1"
     ]
    }
   ],
   "source": [
    "mlp_model = RNN_MLP(\n",
    "    rnn_embed_vocab=NUM_WORDS,\n",
    "    rnn_embed_size=NUM_WORDS//2,\n",
    "    rnn_hidden_size=256,\n",
    "\n",
    "    mlp_embed_input_size=1, \n",
    "    mlp_other_input_size=1, \n",
    "    mlp_embed_vocab=NUM_VENUES, \n",
    "    mlp_embed_size=NUM_VENUES//2, \n",
    "    mlp_hidden_dim1=512, \n",
    "    mlp_hidden_dim2=256, \n",
    "    mlp_out_size=100)\n",
    "    \n",
    "optimizer = torch.optim.SGD(mlp_model.parameters(), lr=1e-2, momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(mlp_model.parameters(), lr=1e-2)\n",
    "mlp_loss, mlp_acc = train(mlp_model, train_dataloader, val_dataloader, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('COMP90051')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "168cf1919776406216c8a7534c836854148f23669d6a513e43ffaae940618bba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
