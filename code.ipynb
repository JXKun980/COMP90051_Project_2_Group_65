{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b8024ca-de50-48a7-83ba-0df4d19842e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336bfd7c-b9bb-4c83-a26b-393f8f183ae6",
   "metadata": {},
   "source": [
    "### 1. Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "03716a1e-1b0a-4b60-b934-545618ec2aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def authors_to_one_hot(a):\n",
    "    one_hot = np.zeros(100).astype(int)\n",
    "    a = np.array(a)\n",
    "    a = a[a < 100]\n",
    "    one_hot[a] = 1\n",
    "    return one_hot\n",
    "\n",
    "# read data from json file\n",
    "train_df = pd.read_json('./data/train.json')\n",
    "train_df.venue = train_df.venue.map(lambda x: -1 if x == '' else x)\n",
    "\n",
    "# Set up training data\n",
    "X_train = train_df.drop(columns=['authors'])\n",
    "y_train = train_df.authors\n",
    "y_train = y_train.map(authors_to_one_hot)\n",
    "\n",
    "# Set up test data\n",
    "X_test = pd.read_json('./data/test.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "058f806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, GridSearchCV\n",
    "\n",
    "# full_df = pd.read_csv('./data/cats.csv')\n",
    "# full_df.SEX = full_df.SEX.map({'M': -1, 'F': 1})\n",
    "# train_df, test_df = train_test_split(full_df, test_size=0.2, random_state=1)\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(train_df.drop('SEX', axis=1)) # fill in\n",
    "# y_train = train_df.SEX # fill in\n",
    "\n",
    "# X_test = scaler.transform(test_df.drop('SEX', axis=1)) # fill in\n",
    "# y_test = test_df.SEX # fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16bcd0b-84f1-4844-90f6-bbe517305f23",
   "metadata": {},
   "source": [
    "### 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a8483",
   "metadata": {},
   "source": [
    "NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce63ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PROLIFIC_AUTHORS = 100\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_name, data_dir, transform=None, target_transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        self.data = pd.read_json(os.path.join(data_dir, file_name))\n",
    "        self.data = self.data.applymap(lambda x: -1 if x == '' else x) # Change null data to -1\n",
    "        \n",
    "        self.x = self.data.drop(columns=['authors']).values\n",
    "        self.y = self.data.authors.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_fixed = self.x[idx,[0, 2]].astype(np.int32)\n",
    "        x_abstract = np.array(self.x[idx,1])\n",
    "        x_title = np.array(self.x[idx,3])\n",
    "        y = np.array(self.y[idx])\n",
    "        y = self.authors_to_one_hot(y)\n",
    "\n",
    "        return x_fixed, x_abstract, x_title, y\n",
    "\n",
    "    def authors_to_one_hot(self, a):\n",
    "        one_hot = np.zeros(NUM_PROLIFIC_AUTHORS).astype(int)\n",
    "        a = np.array(a)\n",
    "        a = a[a < 100]\n",
    "        one_hot[a] = 1\n",
    "        return one_hot\n",
    "\n",
    "train_dataset = CustomDataset('train.json', './data/')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# for _ in range(5):\n",
    "#     _, _, _, y = next(iter(train_dataloader))\n",
    "#     print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f683457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "HIDDEN_DIM1 = 256\n",
    "HIDDEN_DIM2 = 100\n",
    "\n",
    "class MultilayerPerceptronModel(nn.Module):\n",
    "    def __init__(self, n_features, n_classes, hidden_dim1 = HIDDEN_DIM1, hidden_dim2 = HIDDEN_DIM2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(n_features, hidden_dim1)\n",
    "        self.hidden_layer = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.output_layer = nn.Linear(hidden_dim2, n_classes)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        x = x.view(batch_size, -1)  # Flatten image into vector, retaining batch dimension\n",
    "        \n",
    "        x = F.relu(self.input_layer(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.hidden_layer(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        out = self.output_layer(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7703ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tkinter import Y\n",
    "\n",
    "def train(model, train_loader, test_loader, optimizer, n_epochs=10):\n",
    "    \"\"\"\n",
    "    Generic training loop for supervised multiclass learning\n",
    "    \"\"\"\n",
    "    LOG_INTERVAL = 250\n",
    "    running_loss, running_accuracy = list(), list()\n",
    "    start_time = time.time()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(n_epochs):  # Loop over training dataset `n_epochs` times\n",
    "\n",
    "        epoch_loss = 0.\n",
    "\n",
    "        for i, data in enumerate(train_loader):  # Loop over elements in training set\n",
    "\n",
    "            x_fixed, x_abstract, x_title, y = data\n",
    "\n",
    "            logits = model(x_fixed.float())\n",
    "\n",
    "            predictions = torch.round(logits)\n",
    "            train_acc = torch.mean(torch.sum(torch.eq(predictions, y)).float()).item()\n",
    "\n",
    "            loss = criterion(input=logits, target=y)\n",
    "\n",
    "            loss.backward()               # Backward pass (compute parameter gradients)\n",
    "            optimizer.step()              # Update weight parameter using SGD\n",
    "            optimizer.zero_grad()         # Reset gradients to zero for next iteration\n",
    "\n",
    "\n",
    "            # ============================================================================\n",
    "            # You can safely ignore the boilerplate code below - just reports metrics over\n",
    "            # training and test sets\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "            running_accuracy.append(train_acc)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if i % LOG_INTERVAL == 0:  # Log training stats\n",
    "                deltaT = time.time() - start_time\n",
    "                mean_loss = epoch_loss / (i+1)\n",
    "                print('[TRAIN] Epoch {} [{}/{}]| Mean loss {:.4f} | Train accuracy {:.5f} | Time {:.2f} s'.format(epoch, \n",
    "                    i, len(train_loader), mean_loss, train_acc, deltaT))\n",
    "\n",
    "        print('Epoch complete! Mean loss: {:.4f}'.format(epoch_loss/len(train_loader)))\n",
    "\n",
    "        # test(model, criterion, test_loader)\n",
    "        \n",
    "    return running_loss, running_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3f3f637",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "round() got an unexpected keyword argument 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\1-Google Drive\\1-Work\\Projects\\COMP90051\\Project2\\code.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/1-Google%20Drive/1-Work/Projects/COMP90051/Project2/code.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m mlp_model \u001b[39m=\u001b[39m MultilayerPerceptronModel(NUM_FEATURES_FIXED, NUM_PROLIFIC_AUTHORS)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/1-Google%20Drive/1-Work/Projects/COMP90051/Project2/code.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(mlp_model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-2\u001b[39m, momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/1-Google%20Drive/1-Work/Projects/COMP90051/Project2/code.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m mlp_loss, mlp_acc \u001b[39m=\u001b[39m train(mlp_model, train_dataloader, \u001b[39mNone\u001b[39;49;00m, optimizer)\n",
      "\u001b[1;32mc:\\1-Google Drive\\1-Work\\Projects\\COMP90051\\Project2\\code.ipynb Cell 10\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, test_loader, optimizer, n_epochs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/1-Google%20Drive/1-Work/Projects/COMP90051/Project2/code.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m x_fixed, x_abstract, x_title, y \u001b[39m=\u001b[39m data\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/1-Google%20Drive/1-Work/Projects/COMP90051/Project2/code.ipynb#X24sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m logits \u001b[39m=\u001b[39m model(x_fixed\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/1-Google%20Drive/1-Work/Projects/COMP90051/Project2/code.ipynb#X24sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m predictions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mround(logits, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/1-Google%20Drive/1-Work/Projects/COMP90051/Project2/code.ipynb#X24sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m train_acc \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39meq(predictions, y))\u001b[39m.\u001b[39mfloat())\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/1-Google%20Drive/1-Work/Projects/COMP90051/Project2/code.ipynb#X24sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39mlogits, target\u001b[39m=\u001b[39my)\n",
      "\u001b[1;31mTypeError\u001b[0m: round() got an unexpected keyword argument 'dim'"
     ]
    }
   ],
   "source": [
    "NUM_FEATURES_FIXED = 2\n",
    "\n",
    "mlp_model = MultilayerPerceptronModel(NUM_FEATURES_FIXED, NUM_PROLIFIC_AUTHORS)\n",
    "optimizer = torch.optim.SGD(mlp_model.parameters(), lr=1e-2, momentum=0.9)\n",
    "mlp_loss, mlp_acc = train(mlp_model, train_dataloader, None, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adecaeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25793\n"
     ]
    }
   ],
   "source": [
    "def authors_to_one_hot(a):\n",
    "    one_hot = np.zeros(100).astype(int)\n",
    "    a = np.array(a)\n",
    "    a = a[a < 100]\n",
    "    one_hot[a] = 1\n",
    "    return one_hot\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_name, data_dir, transform=None, target_transform=None):\n",
    "        self.data = pd.read_json(os.path.join(data_dir, file_name))\n",
    "        self.data_dir = data_dir\n",
    "        # self.transform = transform\n",
    "        # self.target_transform = target_transform\n",
    "        print(len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "\n",
    "a = CustomDataset('train.json', './data/')\n",
    "    \n",
    "\n",
    "# src: https://androidkt.com/load-pandas-dataframe-using-dataset-and-dataloader-in-pytorch/\n",
    "class MyDataset():\n",
    "    def __init__(self, file_name, data_dir):\n",
    "        train_df=pd.read_json(file_name)\n",
    "        train_df = train_df.map(lambda x: -1 if x == '' else x)\n",
    "        \n",
    "        X_train = train_df.drop(columns=['authors'])\n",
    "        y_train = train_df.authors\n",
    "        y_train = y_train.map(authors_to_one_hot)\n",
    "        \n",
    "        x=X_train.iloc[:,0].values\n",
    "        print(y_train)\n",
    "        y=y_train.values\n",
    "        y = [np.array(arr).astype(int) for arr in y]\n",
    "        \n",
    "        # x=train_df.iloc[:,3].map(lambda x: -1 if x == '' else x).values # WRONG!!!\n",
    "        # y=train_df.iloc[:,0].map(authors_to_one_hot).values\n",
    "    \n",
    "        print(type(train_df.iloc[:,3]).map(lambda x: -1 if x == '' else x))\n",
    "        print(y)\n",
    "\n",
    "        y = [np.array(arr).astype(int) for arr in y]\n",
    "        \n",
    "        self.x_train=torch.tensor(x, dtype=torch.float32)\n",
    "        self.y_train=torch.tensor(y, dtype=torch.float32)\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "   \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_train[idx],self.y_train[idx]\n",
    "\n",
    "# # Define Conv. network (Src: worksheet 8)\n",
    "# class BasicConvNet(nn.Module):\n",
    "#     def __init__(self, out_c1, out_c2, dense_units, n_classes=10):\n",
    "#         super(BasicConvNet, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=out_c1, kernel_size=5)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.conv2 = nn.Conv2d(in_channels=out_c1, out_channels=out_c2, kernel_size=5)\n",
    "#         self.fc1 = nn.Linear(16 * 5 * 5, dense_units)\n",
    "#         self.logits = nn.Linear(dense_units, n_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = x.view(-1, 16 * 5 * 5)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         out = self.logits(x)\n",
    "#         return out\n",
    "\n",
    "\n",
    "# # Define parameters\n",
    "# BATCH_SIZE = 128\n",
    "# OUT_C1 = 8\n",
    "# OUT_C2 = 16\n",
    "# DENSE_UNITS = 256\n",
    "# LR = 1e-2\n",
    "    \n",
    "# Load data\n",
    "# train_ds = MyDataset('./data/train.json')\n",
    "# test_ds = MyDataset('./data/test.json')  #TODO: \n",
    "\n",
    "# train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,shuffle=True)\n",
    "# test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE,shuffle=False)\n",
    "    \n",
    "# # Construct model\n",
    "# conv2D_model = BasicConvNet(OUT_C1, OUT_C2, DENSE_UNITS)\n",
    "\n",
    "# # Start training \n",
    "# optimizer = torch.optim.SGD(conv2D_model.parameters(), lr=LR, momentum=0.9)  #TODO: try adam\n",
    "# conv_loss, conv_acc = train(conv2D_model, train_loader, test_loader, optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb92d44",
   "metadata": {},
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f8a64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6758a0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size) \n",
    "        self.activation = nn.Tanh() \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.activation(self.i2h(combined)) \n",
    "        output = self.h2o(hidden) \n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample(noise=0, noise_chars=\".,;'\"):\n",
    "    # noise: integer denoting the maximum number of distractor characters to add\n",
    "    # noise_chars: inventory of distractor characters\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    # added code to insert distracting nonsense into the string\n",
    "    if noise > 0:\n",
    "        line_prime = line\n",
    "        for i in range(random.randint(0, noise+1)):\n",
    "            line_prime += random.choice(noise_chars)\n",
    "        line = line_prime\n",
    "    # end change\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "n_iters = 80000\n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "noise_level = 0 # change this line (as discussed later)\n",
    "n_hidden = 32\n",
    "learning_rate = 0.005\n",
    "\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "rnn = RNNClassifier(n_letters, n_hidden, n_categories)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "start = time.time()\n",
    "\n",
    "# training algorithm, which takes one instance and performs single SGD update\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    # key step: unroll the RNN over each symbol in the input sequence\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "    # treat the last output as the prediction of the category label\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "    return output, loss.item()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample(noise=noise_level)\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2d9441",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionalGRUClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(AttentionalGRUClassifier, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size) \n",
    "        self.att = nn.Linear(hidden_size, 1) \n",
    "        \n",
    "    def forward(self, input_sequence):\n",
    "        # process the input sequence into a sequence of RNN hidden states\n",
    "        states, _ = self.gru(input_sequence)\n",
    "        # compute attention scores to each RNN hidden state (we use a linear function)\n",
    "        att_scores = self.att(states)\n",
    "        # rescale the attention scores using a softmax, so they sum to one\n",
    "        alpha = F.softmax(att_scores, dim=0)\n",
    "        # compute the \"c\" vector as a weighted combination of the RNN hidden states\n",
    "        c = torch.sum(torch.mul(states, alpha), dim=0)\n",
    "        # now couple up the c state to the output, and compute log-softmax\n",
    "        output = self.h2o(c.view(1, -1)) \n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, alpha\n",
    "\n",
    "model = AttentionalGRUClassifier(n_letters, n_hidden, n_categories)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses_att = []\n",
    "current_loss = 0\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample(noise=noise_level)\n",
    "\n",
    "    model.zero_grad()\n",
    "    output, _ = model.forward(line_tensor)\n",
    "    output = torch.squeeze(output, 1) # remove redundant dimension\n",
    "    loss = criterion(output, category_tensor)\n",
    "    current_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses_att.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('COMP90051')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "168cf1919776406216c8a7534c836854148f23669d6a513e43ffaae940618bba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
